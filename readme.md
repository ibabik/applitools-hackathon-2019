This is my submission to the Applitools Visual AI Rockstar Hackathon. It won a Gold winner award and a monetary prize. Contest participants had to write test automation scripts against a demo web app using two separate approaches: a traditional code-based testing approach using Selenium, WebdriverIO, or Cypress and an image-based testing approach using Applitools Visual AI. 

The traditional tests are arranged in 2 folders: 
  - v1: This contains the first version of tests. If run against V1 of the application, they will pass. If run against V2, some tests will fail. 
  To point the tests at V1 or V2 of the app, set the value of environment variable 'isV1' to true or false respectively. The variable can be found in 'cypress.json'. 
  - v2: These tests can run only against V2 of the app. They are an ammended version of the initial tests. Among the failures of the 1st version, I found both 'new features' (e.g., new placeholders in the input fields, updated wording of input validation errors) as well as genuine bugs (e.g., wrong title of the input form, broken sort order in the Amounts column etc.). I did ammend the tests to support the new features. I did not change the tests that caught the bugs. Instead, I muted these tests and added a comment in the code that the test was muted due to a known issue. At work, we usually do this and put a note in the bug ticket as well, so the tester who verifies the bug-fix will also unmute the test. I find this way better than leaving the tests failing because such tests clutter test results, create noise, and contribute to the reputation of front end tests being slow and flacky. In this test suite each test checks only 1 statement, so muting the tests that fail due to a known bug does not affect test coverage.  
The Visual AI tests require much less lines of code, do not need alteration for the V2 of the test app and self-heal as expected.
